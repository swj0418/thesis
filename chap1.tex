\chapter{Introduction}

% - every transmit has parameters
% - these parameters give a distinct pulse shape
% - but these shapes are ignored after STFT, because power?

\section{Introduction to Ultrasound Beamforming}
 Diagnostic medical ultrasound has its roots in sonar and ultrasonic metal flaw detectors. It is a noninvasive, affordable, portable, and real-time method to characterize the cross-sectional view of soft tissues compared with other imaging modalities such as computed tomography (CT) and magnetic resonance imaging (MRI). The underlying principle of ultrasound is the measurement of time elapsed between sending a signal and receiving its echo; given the sound speed \textit{a priori}, we can thus calculate the distance to an object based on this duration.

 Ultrasound imaging consists of three steps: emitting sound waves (transmit), receiving echoes (receive), and interpreting those responses to form an image. The transmit step is achieved with ultrasonic transducers - devices that convert electricity into ultrasound waves or vice versa. These same transducers are utilized to receive ultrasonic echoes.

 In practice, ultrasound scans are acquired with an array of transducer elements and each transducer element's pulse transmission is preciously timed by a computer. The most basic case of ultrasound imaging is plane wave imaging, where all elements in an array transducer emit the same acoustic pulses at the same time, forming a flat wavefront. After the transmit event, the waves propagate the field of view and are scattered back to the transducers as pulse-echo responses.


 % Channel data per transducer is radio frequency (RF) data (measured in time) for each depth. EXPLAIN??????? I thought it was acoustic energy???? What's actually in it?

 The distance to an object is then calculated as
 $$depth = \frac{sound\,speed * time}{2}$$ % what about depth = t / fs * c /2?
 , accounting for both directions of travel. With this relationship, the time dimension can be translated to the distance or the depth dimension during the processing step. Thus, each transducer has a series of electric signals $v$ for each depth, $y$. With a horizontal or lateral series of $x$ transducers, we have a 2D matrix with each value $v = V[x, y]$ representing the electircal energy (in volts or V) measured by the $x$th transducer element at time $t$. The log-compressed and normalized envelope\footnote{footnotes explaining log compressed envelope w.r.t. dB} of this matrix, with its unit in decibels (dB), becomes an image of width $x$ and height $y$, where each pixel represents the relative intensity in the dynamic range of measured acoustic energy. A typical dynamic range is 60dB. This image modality is called brightness mode, or B-mode. % The lateral dimension may be distinct from the number of transducer elements in cases of horizontal interpolation to increase lateral resolution.

 Ultrasound relies on scattered waves (echoes) which only occur at surfaces between varying acoustic impedance. Conversely, if the field of view contains a single, even medium such as air, no signal is returned to the transducers.

 Now consider the nontrivial example of plane wave imaging of a phantom, which is an artificial composite of materials of various shapes and acoustic impedance. As is the case previously, all elements emit the same pulse at the same timesteps. However, as each pulse wave travels through the composite, it encounters varying impedance, and some of the wave energy gets scattered at various points in depth and at various degrees, depending on the location and the impedance of the component matrials. The returning signals that result from this scattering are used to form a tomographic view of the phantom.

 Compared with plane wave imaging, focused transmit provides better signal-to-noise ratios. The rationale for focused imaging is that responses from adjacent transducer elements are more relevant than those farther away. In practice, focus in ultrasound requires a subgroup of the total transducer array to form a single 1D depth-signal series, as opposed to one series for each element. A set of time-delayed (focused) transducer waves are called a beam. Focused imaging maximizes signal, minimizes noise, and results in a higher signal-to-noise ratio. Beamforming can also be viewed as a method for spatial filtering, it gives us spatial selection over where the energy returns from.
% TODO what is the rationale for focused imaging?

 To achieve focusing, we first select a subset of transducers (called an \textit{aperture}) and slide the selection by one element for $num\_beams$ times, where $num\_beams$ is the configurable number of beams in the overall array. This results in a new channel/aperture dimension to our data, in addition to the depth and lateral ones. We call this new type of data matrix \textit{channel data}. % We use 0  [Where exactly does the 0-padding sliding start? Like 0th sliver comes from transducers \#0-64 or \#-32-32?].

 Within each subset, we need to send out a focused wave of a curved wavefront by taking advantage of wave interference. The superimposition of waves can cause constructive or destructive interferences, depending on their relative phases and amplitudes upon contact. We preset a focus (typically controlled by a knob on an ultrasound machine), from which we then use a Pythagorean-like calculation to compute the delays we need.% The ultrasound machine typically applies the delays automatically. % [How exactly does focusing work? What wavefront shape do we actually want, given focus?].

 The processing of channel data in order to form an image is called beamforming. The most basic method of beamforming is delay and sum (DAS). We time-delay the data after receiving in order to adjust for the path length differences between the returning wavefront and the transducer elements. After applying delays, we finally operate on the channel dimension. The dimension of our post-delayed channel data is $[depth, elements\_per\_channel, num\_beams]$. To form each beam (vertical slice in the final image), we collapse its channel dimension by summing all 1D transducers responses in its aperture group, resulting in a new data matrix of size $[depth, num\_beams]$, called beamformed radio frequency (RF) data. The log compressed envelope (amplitude profile) of the normalized beamformed RF data is the resulting image.
 % TODO: explain the relationship between num\_total\_elements => num\_beams?


\section{Challenges in Ultrasound Beamforming}

% TODO: Li 2016 visualization has sources of reverb.
Although widely accepted, DAS beamforming is not an ideal method for clinical application due to the presence of many noises or artifacts, of which we present two: off-axis scattering and reverberation.

% TODO: move scattering up?
Underlying these two artifacts is the basic mechanism of ultrasound - scattering. Scattering describes the reflection of an acoustic wave as it encounters the boundary between matters of differing impedance. There are many scatters (boundaries) in the field of view. We assume that a wave scatters once before returning to the transducer array and that a transducer subarray emits a straight wave aimed linearly down. However, these assumptions are not always true. The unintended behaviors of scattering lead to artifacts such as off-axis scattering and reverberation clutter. % For example, speckles are formed by interference among scattered waves which stray from the ROI. Even though speckles are not considered an artifact, there are other properties of scattering that are.


\subsection{Off-Axis Scattering}

The first such artifact is off-axis scattering. We typically assume that pulse-waves propagate downward, but in reality, pulse waves exhibit diffraction when emitted by a transducer beam. The diffraction resembles the way sound travels. Using an analogy, when person A shouts a secret message facing person B, a bystander C can often hear the message as well (if less clearly). In ultrasound, this phenomenon can be illustrated by beam plots - the normalized far-field magnitude of the transmit pressure versus observation angle. The acoustic pressure we want to focus on is the mainlobe, and the sidelobes are the diffractions that dilute the energy and cause off-axis echoes that degrade the image.


\subsection{Reverberation}

Another cause of image degradation is reverberation or multipath scattering. We assume that when a wave encounters a boundary, it is reflected back to and only to the transducers. However, in reality, the scattered wave can travel in all directions. In addition, the divergent scattered waves can be further scattered by boundaries outside the ROI of the emitting beam. As a result of bouncing around in the field of view, the returning signal gets registered to deeper depths because it takes longer to return to the transducer.

% The multipath-scattered responses interference with the desired vertical responses from the ROI and degrades the resulting image.


% TODO: Describe the Region of Interest (ROI) with a figure like Kaz's fig 1.9 (without the porcupine).

\section{Noise Suppression Algorithms}

Many algorithms have been developed to address ultrasound artifacts. Earlier methods include Tissue Harmonic Imaging, Time-Reversal Technique, Second-Order Ultrasound Field, Minimum-Variance Beamforming, coherence factor, generalized coherence factor, and Short-Lag Spatial Coherence\cite{slsc}. Aperture Domain Model Image REconstruction (ADMIRE) developed by Byram and Dei \cite{admire2015}. Machine learning approaches tend to work well, but they are too computationally demanding to be real-time, although progress has been made to improve its performance.

In addition, studies have shown that deep neural networks are effective in suppressing noise sources. Of particular relevance is the application of multilayer perceptrons (MLPs) in the aperture domain or channel dimension \cite{luchies_tmi_2018}. Lastly, convolutional neural networks (CNNs) have also been used in biomedical imaging in general \cite{unet} and various ultrasound imaging modalities \cite{van_sloun_review}. In terms of ultrasound beamforming, work has been done using CNNs to learn the entire beamforming process \cite{hyun_uffc_2019}.

% "Hyun et al. [44] propose deep convolutional neural networks for joint beamforming and speckle reduc- tion. Rather than applying the latter as a post-processing technique, it is embedded in the beamforming process itself, permitting exploitation of both channel and phase information that is otherwise irreversibly lost."


% TODO introduce why the frequency domain.
\section{Contribution} % Contribution
% No one has applied Convolutional Neural Networks on the STFT-domain data.
This thesis provides a pioneering study of the effectiveness of convolutional neural networks on suppressing off-axis scattering in the frequency domain. I first show that models with both convolutional layers and fully-connected layers can approximate the performance of MLPs in suppressing noise and improving CNR. I further study the mechanism of convolution on the model performance and find that fully-convolutional networks (FCNs) do not perform as well as MLPs but outperform the DAS benchmark. By investigating the effect of kernel size and the number of convolutional layers on CNR, I find that convolutions do not solve the noise suppression problem better than MLPs; in fact, FCNs approximate MLPs by hainvg a large receptive field to cover either the complex component or the entire input space.
%
% The aim of this thesis is to investigate the effectiveness of convolutional neural networks (CNNs) in suppressing off-axis scattering and reverberation clutter. Particularly, I apply CNNs to signals in the aperture short-time Fourier transform (STFT) domain to avoid having to train for for different pulse shapes, depth dependent attenuation, and other pulse parameters that may vary across patients and even across probes as they age. I will discuss the rationale in detail in Chapter 2.

\section{Organization}
Chapter 2 discusses background on deep neural networks and related work on noise suppression with deep learning and CNN-based beamforming. Chapter 3 describes the training data generation process and signal grouping. Chapter 4 explains the CNN architectures and the training pipeline, including a random hyperparameter search technique. Chapter 5 details the beamforming pipeline for evaluation. Chapter 6 addresses the limitations of this work, discusses the results, and concludes the thesis with potential future work.
